Scaling Instruction-Finetuned Language Models
Why does the Flan paper observe diminishing but still positive returns when adding more than ~282 instruction finetuning tasks, and how does this inform strategies for building future multi-task datasets?

Training language models to follow instructions with human feedback
How well does RLHF-trained InstructGPT generalize to instructions from users or domains that are very different from the training prompts, and what strategies could further improve cross-domain robustness without compromising alignment?
