Paper: Training language models to follow instructions with human feedback (InstructGPT)
Question:
Given that InstructGPT is aligned mostly to a small set of human labelers and API users, how should future work address alignment among diverse perspectives (cultural, linguistic, and ethical) to avoid unintended biases?

Paper: Scaling Instruction-Finetuned Language Models (FLAN)
Question:
How does including even a small number of Chain-of-Thought (CoT) datasets enable zero-shot reasoning on unseen tasks, and why would instruction finetuning without CoT data actually degrade reasoning performance?