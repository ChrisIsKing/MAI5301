FlashAttention: 
How does FlashAttention reduce memory access overhead?

Longformer: 
How does the Longformer’s attention mechanism improve efficiency?
