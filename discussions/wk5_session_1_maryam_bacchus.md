Questions

Paper 1 - Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
1. Does sparse expert routing introduce additional latency or unpredictability during inference compared to dense models? 

Paper 2 - Mixtral of Experts 
1. How sensitive is Mixtral to training data distribution, do experts specialize in certain languages or domains?