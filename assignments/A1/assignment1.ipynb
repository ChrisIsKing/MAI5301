{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Build Vocabulary\n",
        "\n",
        "import re\n",
        "\n",
        "words = \"\"\"hello, world. how are you? This is a test. Cat, rat, dog, bat, elephant, can,\n",
        "fish, row, column, food, eat, speak, drink, run, jump, skip, ---, climb, over,\n",
        "under, above, sunny, rainy, <|endoftext|>, <|unk|>\"\"\"\n",
        "\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', words)\n",
        "# Lowercase all words for handling differences in capitalization.\n",
        "preprocessed = [item.lower() for item in preprocessed]\n",
        "# Add capitalized version of all words and append to vocab list\n",
        "preprocessed += [item.capitalize() for item in preprocessed]\n",
        "\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "preprocessed = sorted(set(preprocessed))\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(preprocessed)}\n",
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeaZoblk-v91",
        "outputId": "d0e841da-cbc8-47c4-da35-8d0045fa70cd"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(',', 0)\n",
            "('-', 1)\n",
            "('--', 2)\n",
            "('.', 3)\n",
            "('<|endoftext|>', 4)\n",
            "('<|unk|>', 5)\n",
            "('?', 6)\n",
            "('A', 7)\n",
            "('Above', 8)\n",
            "('Are', 9)\n",
            "('Bat', 10)\n",
            "('Can', 11)\n",
            "('Cat', 12)\n",
            "('Climb', 13)\n",
            "('Column', 14)\n",
            "('Dog', 15)\n",
            "('Drink', 16)\n",
            "('Eat', 17)\n",
            "('Elephant', 18)\n",
            "('Fish', 19)\n",
            "('Food', 20)\n",
            "('Hello', 21)\n",
            "('How', 22)\n",
            "('Is', 23)\n",
            "('Jump', 24)\n",
            "('Over', 25)\n",
            "('Rainy', 26)\n",
            "('Rat', 27)\n",
            "('Row', 28)\n",
            "('Run', 29)\n",
            "('Skip', 30)\n",
            "('Speak', 31)\n",
            "('Sunny', 32)\n",
            "('Test', 33)\n",
            "('This', 34)\n",
            "('Under', 35)\n",
            "('World', 36)\n",
            "('You', 37)\n",
            "('a', 38)\n",
            "('above', 39)\n",
            "('are', 40)\n",
            "('bat', 41)\n",
            "('can', 42)\n",
            "('cat', 43)\n",
            "('climb', 44)\n",
            "('column', 45)\n",
            "('dog', 46)\n",
            "('drink', 47)\n",
            "('eat', 48)\n",
            "('elephant', 49)\n",
            "('fish', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "-JPmEJ2UZdlh"
      },
      "outputs": [],
      "source": [
        "# Build Tokenizer (copied from book)\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text, allowed_special=False):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids, allowed_special=False):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(vocab)\n",
        "text = \"hello, how are you?\"\n",
        "text2 = \"Hello, how are you?\"\n",
        "# encode text\n",
        "encoded_text = tokenizer.encode(text)\n",
        "encoded_text2 = tokenizer.encode(text2)\n",
        "print(encoded_text, \"\\n\", encoded_text2)\n",
        "\n",
        "# Decode text\n",
        "decoded_text = tokenizer.decode(encoded_text)\n",
        "decoded_text2 = tokenizer.decode(encoded_text2)\n",
        "print(decoded_text, \"\\n\", decoded_text2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vrQhmQoBkvg",
        "outputId": "70fba221-b128-44d5-9194-b1ef9aa777cf"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[52, 0, 53, 40, 68, 6] \n",
            " [21, 0, 53, 40, 68, 6]\n",
            "hello, how are you? \n",
            " Hello, how are you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLS8v-l3FJc8",
        "outputId": "82566bfd-9287-4d62-c60d-4e8c9681fe72"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tests comparing tokenizer output with tiktoken\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "tokenizer_tiktoken = tiktoken.get_encoding(\"gpt2\")\n",
        "text = \"Hello, how are you?\"\n",
        "text2 = \"hello, how are you?\"\n",
        "\n",
        "integers = tokenizer_tiktoken.encode(text)\n",
        "integers2 = tokenizer_tiktoken.encode(text2)\n",
        "\n",
        "print(integers, \"\\n\", integers2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaA5h9VqFDDD",
        "outputId": "2ca1b2e1-c98d-43a8-a436-4732dd966d68"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 703, 389, 345, 30] \n",
            " [31373, 11, 703, 389, 345, 30]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing unknown string on custom tokenizer and tiktoken\n",
        "\n",
        "unknown_string = \"ikorrdd\"\n",
        "\n",
        "# Encoding\n",
        "print(tokenizer.encode(unknown_string))\n",
        "print(tokenizer_tiktoken.encode(unknown_string))\n",
        "\n",
        "# Decoding\n",
        "print(tokenizer.decode(tokenizer.encode(unknown_string)))\n",
        "print(tokenizer_tiktoken.decode(tokenizer_tiktoken.encode(unknown_string)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qcvzk_LfGzXK",
        "outputId": "358e0304-a7ca-4365-cec7-6b867dc632b4"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5]\n",
            "[1134, 38890, 1860]\n",
            "<|unk|>\n",
            "ikorrdd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement Dataset for DataLoader - copied from book\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={'<|endoftext|>'})\n",
        "\n",
        "        # Use a sliding window to chunk into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "fkzaANXXKp_S"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloader implementation\n",
        "dataset = GPTDatasetV1(words, tokenizer, max_length=4, stride=1)\n",
        "dataloader = DataLoader(dataset, batch_size=2)\n",
        "\n",
        "first_batch = iter(dataloader)\n",
        "print(next(first_batch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4onjYok6NqnE",
        "outputId": "cae9af6d-a8d2-4627-9d01-1808f0b78836"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[52,  0, 67,  3],\n",
            "        [ 0, 67,  3, 53]]), tensor([[ 0, 67,  3, 53],\n",
            "        [67,  3, 53, 40]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QRvSbevwLKde"
      }
    }
  ]
}