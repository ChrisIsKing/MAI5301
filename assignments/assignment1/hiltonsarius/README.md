1) Why causal masking is necessary for language models

Causal masking is essential in autoregressive language models (e.g., GPT) because they are trained to predict the next token using only past context. In a transformer, self-attention would naturally allow every token to attend to every other token in the input sequence, including tokens that come later (future tokens). Without causal masking, the model could “cheat” during training by looking ahead at the correct next words, which leads to unrealistically low training loss and poor real-world generation performance. Causal masking solves this by applying a triangular mask to the attention matrix so that token t can only attend to tokens 1…t (not t+1…n). This forces the model to learn realistic language dependencies the same way it will operate at inference time—generating text step-by-step without access to future tokens. It ensures training and inference conditions match, preventing information leakage.

2) The purpose of multiple attention heads

Multiple attention heads allow transformers to learn different relationships in parallel within the same layer. A single attention head produces one set of attention weights, which usually focuses on one dominant pattern at a time. But language contains many simultaneous patterns—syntax, semantics, long-range references, word order, formatting, and more. Multi-head attention addresses this by splitting the model’s embedding space into multiple subspaces, then computing attention independently in each head using different learned projections (separate Q, K, V matrices). Each head can specialize in a different type of dependency, such as subject–verb agreement, pronoun resolution, nearby token smoothing, or attending to punctuation/structure. Afterward, outputs from all heads are concatenated and mixed, giving the model a richer representation. From a developer perspective, it’s like having several “experts” look at the same sentence simultaneously, each extracting useful signals, improving expressiveness, stability, and overall performance.

3) How attention weights reveal the model’s focus

Attention weights show how strongly a model links the current token to earlier tokens when computing its representation. In self-attention, each token produces a query vector that compares against key vectors for other tokens, generating scores that are normalized via softmax. The resulting attention weights act like a distribution: high weight on a token means the model is prioritizing information from that position. When visualized as heatmaps, attention weights can reveal patterns such as whether the model relies on nearby context, tracks long-range dependencies, or connects related words (e.g., pronouns to their antecedents). For example, when predicting a verb, a head may attend heavily to the subject token, showing syntactic focus. However, attention is not a perfect explanation of model reasoning: information is distributed across many heads and layers, and feed-forward networks also store and transform features. Still, attention weights are a valuable interpretability tool for diagnosing model behavior.