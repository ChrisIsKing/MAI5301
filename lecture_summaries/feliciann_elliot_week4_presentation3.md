### Problem Addressed and Problem Importance
Modern large language models depend on massive pretraining datasets, but the data used to train most high-performing models remains proprietary or poorly documented. This lack of transparency is a significant issue for researchers who want to study how data composition, filtering decisions, and deduplication strategies affect model behavior, bias, and generalization (Soldaini et al., 2024). Even though open-weight models have become more common, they are often released without the corresponding training data or reproducible curation pipelines. This means researchers cannot independently validate or analyze these models, which limits rigorous research on memorization, benchmark contamination, and the ethical implications of large-scale data collection (Soldaini et al., 2024).

### State of Related Works in This Topic
Earlier open datasets such as C4 and The Pile provided important benchmarks for large-scale language modeling, but their sizes were not sufficient for training modern frontier-scale models (Soldaini et al., 2024). More recent datasets like RedPajama and Falcon achieved greater scale but relied heavily on Common Crawl data, which limited domain diversity and increased sensitivity to web-specific noise. Other efforts such as ROOTS emphasized multilingual coverage but did not provide a sufficiently large English corpus for training robust English-centric models. Overall, prior work highlighted a tradeoff between scale, quality, and transparency, with few datasets successfully addressing all three dimensions at once (Soldaini et al., 2024).

### Proposed Solution
The authors introduce Dolma, an open corpus containing approximately three trillion tokens of English text curated from six primary sources, including Common Crawl web data, scientific literature, code repositories, books, encyclopedic content, and social media discussions (Soldaini et al., 2024). To build Dolma, the authors developed a scalable, open-source data curation toolkit capable of filtering hundreds of terabytes of raw data using language identification, heuristic quality rules, toxicity detection, personally identifiable information masking, and multi-stage deduplication. The authors validated these design choices through ablation experiments that demonstrate how specific web quality filters, Reddit formatting strategies, and stacked filtering stages improve downstream model performance (Soldaini et al., 2024). The dataset's effectiveness is further confirmed by training OLMo-1B models on Dolma, which achieve competitive results relative to other open models on a range of benchmarks.

### Drawbacks and Limitations
Despite its scale and transparency, Dolma has some limitations. The dataset focuses only on English-language data, which reinforces the dominance of English in large language model research (Soldaini et al., 2024). The dataset's immense size also makes comprehensive manual inspection impossible, which means harmful or biased content might persist despite filtering efforts. The validation experiments are primarily conducted at the one-billion-parameter scale, so some findings may not fully generalize to substantially larger models. Legal and ethical uncertainties surrounding copyright and data governance also remain unresolved, as the regulatory landscape governing large-scale data use continues to evolve (Soldaini et al., 2024).

### Future Research
The authors propose extending Dolma to include additional languages in order to reduce English-centric bias and support multilingual modeling research (Soldaini et al., 2024). Future work also includes refining data formatting strategies for complex sources such as threaded social media conversations and further exploring optimal data mixing ratios. By releasing both the dataset and the curation toolkit, the authors hope to encourage the research community to experiment with alternative filtering thresholds, governance mechanisms, and consent-aware data collection frameworks to improve transparency and accountability in language model pretraining.

### References
Soldaini, L., Kinney, R., Bhagia, A., Schwenk, D., Atkinson, D., Authur, R., Bogin, B., Chandu, K., Dumas, J., Elazar, Y., Hofmann, V., Jha, A. H., Kumar, S., Lucy, L., Lyu, X., Lambert, N., Magnusson, I., Morrison, J., Muennighoff, N., â€¦ Lo, K. (2024, January 31). Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv. https://arxiv.org/abs/2402.00159