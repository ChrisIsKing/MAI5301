Emergent Abilities of Large Language Models

This paper examines the idea of emergent abilities in large language models (LLMs), arguing that some capabilities only appear once models reach a certain scale and cannot be predicted by simply extrapolating trends from smaller models. The authors define an emergent ability as one that is essentially absent in small models but suddenly becomes strong in larger ones. They survey many examples across few-shot prompting tasks (e.g., arithmetic, multilingual QA, TruthfulQA, MMLU) and augmented prompting or finetuning strategies such as chain-of-thought, instruction tuning, and scratchpads. A key takeaway is that scale measured by training compute or parameters has enabled new behaviors, raising the possibility that further scaling may unlock even more abilities.

That said, the paper is mostly descriptive rather than explanatory. While the its extensive and well-organized, it does not offer a convincing causal theory for why emergence happens, we simply dont know. In many cases, the apparent “sudden jump” could partly be an artifact of evaluation metrics (e.g., exact match or accuracy) that hide gradual improvements in underlying probabilities. The authors acknowledge this, but the analysis of cross-entropy loss does not fully resolve the issue. Another concern is that emergence is framed largely around scale, even though the paper itself admits data quality, architecture, and training objectives can shift or lower emergence thresholds. This weakens the claim that emergence is fundamentally about scale rather than about mismatched experimental setups.

A challenging question is whether emergence is a real property of model cognition or just a byproduct of how we design benchmarks and measure success. Also, the paper does not clearly distinguish between genuinely new capabilities and better elicitation of latent ones through prompting tricks. Still, as a survey, it is valuable: it crystallizes an important phenomenon and sets a concrete research agenda, even if the core concept remains somewhat fuzzy and undertheorized.