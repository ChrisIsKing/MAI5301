####Problem Addressed and Problem Importance
The main question asked was: Given a fixed FLOPs budget, how should one trade off model size and the number of training tokens. Researchers wanted to know how they could optimally train their large language model such that they get the most returns on investment. This was an important question because resources are limited for most companies, and training large models was a significant company expense that can only done once. Understanding the optimal allocation of compute between model parameters and training data would lead to better cost saving decisions.

####State of Related Works in This Topic
At the time, a series of large language models had been released, with "impressive performance" on many tasks. However, the compute and energy costs for training those models remained substantial. As the model size increases, so does the compute and energy requirements. Since resources were limited and it was only typically feasible to train models once, there needed to be some metric to determine the optimal model size to train for the given budget. This metric would model the scaling behaviour and guide resource allocation decisions. Prior work had explored various aspects of neural network scaling, but there was no formal understanding of the trade offs between model size and training data.

####Proposed Solution
In "Scaling Laws for Neural Language Models" (Kaplan et al., 2020), the OpenAI researchers found a relationship among compute, model size and data size. For optimally compute efficient training, they found that most of the increase should go towards increased model size. "Performance improves predictably as long as we scale up model parameters and dataset size in tandem". They showed this relationship can be used to model outcomes and offer clarity in training models. This was accomplished by looking at the average loss over many training iterations. The research found that at a certain point, you can give it more data, but that loss would still be constant. At that point, just stop training the model, as the model had reached the point where loss is constant. In "Training Compute-Optimal Large Language Models" (Hoffmann et al., 2022), the DeepMind paper, there was a different relationship among model size and training tokens. They found that cost is proportional to model size, and that model size and training tokens should be scaled equally given increased compute. This was represented through mapping the model size and token count as a function of loss and analysing the data. They used this analysis to show that OpenAI was undertraining their models. specifically, that models like GPT-3 were much larger than optimal for their compute budget, and should have been trained on more tokens.

####Drawbacks and Limitations
A limitation of the scaling law is that OpenAI's way of measuring success may tend towards the natural entropy of natural language because it uses cross-entropy loss. " At this point we do not know which of our results depend on the structure of natural language data, and which are universal." (Kaplan et al., 2020). While DeepMind's proposal also uses cross-entropy evaluation, because they mapped the token size and model size back to loss, they were able to better evaluate the outcome in a more mathematically rigorous and comprehensive way. However, both approaches still rely on loss as the primary metric, which may not fully capture downstream task performance or practical utility. 

####Future Research
In (Hoffmann et al., 2022), it was observed that projections from very small models lead to different predictions than those from larger models. For future works, the researchers suggested analysing scaling behaviour in smaller models, which may be optimal for large FLOP budgets. This raised important questions about whether scaling laws derived from smaller models can reliably predict the behavior of much larger models, and whether there are training process improvements that could optimize the compute training token trade off.