####Problem Addressed and Problem Importance

The two papers by Brown et al. (2020) and Wei et al. (2022) asked the question about emergent abilities of large language models and how you can use them to perform specific tasks without the need for fine tuning. Specifically, they explored how we can develop general purpose models after just training the model once with hundreds of billions of parameters. This was important because understanding the limits of these abilities helped companies understand their models and their capabilities, potentially giving them a competitive edge over their competitors. Also, fine-tuning models for every task was expensive and impractical, requiring thousands of labeled examples and significant computational resources for each new application.

####State of Related Works in This Topic
At the time these papers were being written, Transformers were replacing neural networks, specifically RNNs. This model was being used because it was task agnostic, however it still required large task specific datasets and task-specific fine-tuning to train the model on how to perform the task. This was impractical to be repeated for every single new task. Prior work had established scaling laws showing that model performance improved predictably with size, but the qualitative nature of capability changes at larger scales remained unclear.

####Proposed Solution
Large language models efficiently use in context information to perform tasks. The two papers talk about how as large language models grow in size and use more parameters, emergent abilities occur. The paper by (2020) looked at what you can do with those emergent abilities and how you can apply them in a general purpose way to complete specific tasks, without the model needing to be retrained (fine tuned).Specifically, they trained GPT-3, a 175-billion parameter model, and tested it under three conditions: zero-shot (instruction only), one-shot (single example), and few-shot (multiple examples, typically 10-100). The paper looked at how you can use the model context to "train" models to do these tasks. With enough examples (few shot), the model is able to learn the pattern behind task completion and perform those tasks with acceptable accuracy. It proposed that a model can learn and be general purpose once you supply it with relevant examples. The model will learn patterns from those examples then you can ask it questions. It is different from fine tuning the model by not changing the gradients and weights instead, it leverages patterns recognized purely through text interaction at inference time. The paper by Wei et al. (2022) on "Emergent Abilities of Large Language Models" formalized the concept of emergence, defining an ability as emergent if it is "not present in smaller models but is present in larger models." They documented over 100 examples where a large language model gets significantly large, it's able to do complex tasks. The performance will jump from almost zero to excellent performance. This represents drastically increased accuracy that cannot be predicted simply by extrapolating from smaller model performance.

####Drawbacks
One limitation of context learning is that it is always limited by hardware. Context is stored in the cache in a model. The context window is limited to the KV cache  size. Also, with more context across different topics and unrelated ideas, the model starts to hallucinate as it's trying to find patterns among those things. This is where prompt engineering comes into play to carefully structure examples and maintain relevance. Additionally, because the model was trained on language, GPT-3 still struggled with some arithmetic tasks like multiplication.

####Future Works
The researchers suggested that while we only observer emergent abilities at a certain large scale, it is possible that it could be achieved at a smaller scale. The researchers posit whether better techniques (or scaling) could be developed to predict when emergent abilities would occur. 