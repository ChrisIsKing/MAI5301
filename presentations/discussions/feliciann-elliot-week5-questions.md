Questions

1. Mixtral of Experts
Would Mixtralâ€™s routing mechanism work equally well on non-autoregressive tasks, such as sequence-to-sequence models (e.g., translation) or encoder-only models (e.g., classification)?

2. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
Does sparse expert routing introduce additional latency or unpredictability during inference compared to dense models?