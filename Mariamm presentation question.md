Paper: Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM
How does Megatron-LMâ€™s tensor model parallelism improve training large language models compared to standard data parallelism?

Paper: LLaMA: Open and Efficient Foundation Language Models
How does LLaMA achieve strong performance with fewer parameters compared to other large language models?