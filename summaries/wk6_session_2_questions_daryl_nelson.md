GLU Variants Improve Transformer: 

Why might replacing a traditional activation function (like ReLU/GELU) with a gated linear unit (GLU) variant improve a Transformerâ€™s performance, even when the overall number of parameters and computation stays the same?



GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints: 

What underlying trade-off does grouped-query attention (GQA) implicitly exploit between model capacity and inference efficiency, and why might this trade-off become more advantageous as models scale larger?