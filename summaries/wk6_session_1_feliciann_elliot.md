Questions

Paper 1 - RoFormer: Enhanced Transformer with Rotary Position Embedding

1. The paper emphasizes RoPE’s “decaying inter-token dependency.” Why could this decay be beneficial for language understanding, even though some long-distance dependencies are important?

Paper 2 - Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation

2. What inference can you make about the relationship between computational efficiency and extrapolation ability when using ALiBi instead of sinusoidal or learned positional embeddings?