ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING

Enhanced Transformer with Rotary Position Embedding proposes a new way to improve how transformers handle positional information in sequences. Transformers need some way to know the order of tokens because self-attention on its own does not encode this, and previous approaches often just add fixed or learned position embeddings. The authors investigate several of these methods and then introduce Rotary Position Embedding (RoPE), which uses rotation matrices to encode positions so that relative positional dependency is naturally built into the self-attention mechanism itself. This is different from older additive methods because RoPE directly uses rotation of query and key vectors to encode both absolute and relative position effects.

In terms of methodology, they describe how RoPE works with the rotation matrix to ensure that the inner product of query and key vectors depends on relative distance, and why this property is helpful. The paper also discusses how RoPE gives flexibility over sequence length and decaying dependencies as distance grows. They evaluate RoFormer, the enhanced transformer using RoPE, on long text classification benchmarks and show it consistently outperforms alternatives, which suggests the goals were met. A theoretical analysis is also given to support empirical results.

Overall, this work is relevant because positional encoding remains a challenge in transformer models, and RoPE has since been adopted in popular libraries like Huggingface, showing its practical influence
