GLU Variants Improve Transformer
This paper addresses a key but often under-emphasized architectural issue in transformers: while research attention typically focuses on improving self-attention, a large portion of a transformer’s parameters and compute lies in the position-wise feed-forward network (FFN) sublayer, and the choice of FFN activation can significantly affect overall model performance. Standard transformers use a two-layer MLP FFN with an activation such as ReLU or GELU, but these activations may not be the most effective way to allocate model capacity. This problem is important because transformer models are trained at immense computational cost; even small improvements to efficiency or prediction quality at the layer level can compound across many layers and tokens, yielding meaningful gains in perplexity and downstream task accuracy. Improving the FFN is therefore a high-impact opportunity to enhance transformer performance without changing the attention mechanism itself.
Related work includes the original Transformer architecture introduced by Vaswani et al., where FFNs consist of a linear projection, a pointwise nonlinearity (initially ReLU), and a second linear projection. Subsequent transformer variants widely adopted GELU because it improves optimization stability and performance in large-scale language models. Meanwhile, Gated Linear Units (GLUs) were already known from earlier deep learning research (especially convolutional sequence models), where a multiplicative gate controls information flow, similar in spirit to gating in LSTMs/GRUs. Before Shazeer’s paper, however, GLU-style gating was not a standard default inside transformer FFNs, leaving open the question of whether gated activations could systematically outperform GELU/ReLU in transformer settings.
The proposed solution is to replace the standard FFN activation with GLU-based variants, where the FFN computes two linear projections of the input: one provides values, and the other provides a gate after applying a nonlinear activation. The FFN output is produced by element-wise multiplication of these two streams before the final projection, yielding variants such as ReGLU, GEGLU, and SwiGLU depending on which activation (ReLU, GELU, or Swish/SiLU) is used on the gating branch. The key guiding insight is that multiplicative gating creates a richer, conditional transformation than a single nonlinearity: instead of merely transforming features, the model learns to dynamically suppress or amplify them based on context. This increases expressiveness and improves how the FFN allocates capacity, leading to consistent empirical gains in transformer training outcomes.
A limitation of the approach is that gated FFNs require additional linear projections and element-wise multiplications, which can alter computational efficiency and complicate optimized implementations. While improvements may be strong in accuracy, the compute/memory tradeoffs depend on hardware and kernel optimizations. Additionally, the paper provides mostly empirical validation and does not fully explain why specific variants (like GEGLU or SwiGLU) outperform others, leaving the mechanism somewhat under-theorized. Potential future research directions include developing stronger theoretical explanations for gating benefits, studying how GLU variants behave under scaling (very large models and longer contexts), designing learned or adaptive gating functions, and extending these FFN improvements to multimodal transformers and efficient long-context architectures. Overall, the paper’s main contribution is showing that FFN design is a powerful lever in transformer performance, and that simple gated activation substitutions can yield meaningful improvements across tasks.

GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints
This paper addresses a practical and increasingly urgent problem in large language model (LLM) deployment: decoder inference is bottlenecked by memory bandwidth, especially because at every generation step the model must load parameters and repeatedly read/write the key-value (KV) cache for attention. Standard multi-head attention (MHA) uses separate key and value projections per head, which improves modeling quality but makes inference expensive due to large KV caches. Prior work proposed multi-query attention (MQA), which keeps multiple query heads but shares a single key and value head, drastically reducing KV cache size and speeding up decoding. However, MQA can cause quality degradation and training instability, and it is not always feasible to retrain a new model from scratch just to gain inference efficiency—especially when organizations already have expensive multi-head checkpoints that they want to reuse. This problem is important because LLMs are increasingly used in real-time settings (chatbots, assistants, retrieval systems), where latency and throughput directly translate into cost, user experience, and hardware requirements. 
The state of related work includes the transformer baseline with MHA, and inference-efficiency research emphasizing that autoregressive decoding is memory-bound. MQA (originally highlighted in earlier work by Shazeer) is known to shrink the KV cache and speed up inference, and some major models adopted it, but many strong model families and publicly released checkpoints were still trained with multi-head attention. Thus, practitioners faced an unattractive choice: keep high-quality MHA checkpoints but pay high inference cost, or train new MQA models and risk quality loss. This gap created a clear need for a method that can convert existing MHA models into faster variants with minimal retraining cost while keeping quality close to the original. 
The authors propose two main contributions. First, they introduce a recipe for uptraining: converting an existing MHA checkpoint into an MQA-style checkpoint and then continuing pretraining for a small fraction of the original training steps (they report roughly ~5% of the original pretraining compute). The conversion step mean-pools the original multi-head key and value projection matrices into shared key/value projections, then the short uptraining phase allows the model to adapt to this new attention structure. Second, they propose Grouped-Query Attention (GQA), a generalization that interpolates between MHA and MQA by dividing query heads into groups, where each group shares a single key and value head. In their notation, GQA-G means G groups of query heads sharing KV heads; GQA-1 equals MQA, while GQA-H equals MHA (H = number of heads). The key insight is that you do not need a strict binary choice between “full multi-head quality” and “single KV-head efficiency.” Instead, by choosing an intermediate number of KV heads, GQA can achieve quality close to MHA while retaining inference speed close to MQA, because KV cache size and memory traffic scale with the number of KV heads. 
One limitation of the proposal is that although uptraining is far cheaper than full pretraining, it still requires additional compute, data access, and training infrastructure, which may be non-trivial for some users of open checkpoints. Another drawback is that while GQA typically preserves quality well, the best number of groups is a hyperparameter dependent on model size, task mix, and latency constraints—so practical deployment requires experimentation. Finally, the paper largely focuses on language-model settings and does not fully resolve broader architectural questions like cross-layer KV sharing or synergy with other cache-reduction methods. Looking forward, promising directions for future research include: (1) optimizing automatic selection of GQA grouping given latency/quality targets, (2) combining GQA with more aggressive KV cache compression strategies, (3) studying how GQA interacts with long-context training and retrieval-augmented generation, and (4) extending the conversion/uptraining approach to other architectures and modalities. Overall, the paper makes a valuable contribution by showing that efficient inference attention variants can be obtained from existing multi-head checkpoints with minimal additional training, and that GQA offers a flexible tradeoff curve between quality and speed rather than forcing a single fixed design choice. 


