**The Pile: An 800GB Dataset of Diverse Text for Language Modeling**

The Pile prioritizes data diversity over aggressive filtering. What are the risks of this design choice, and why might the authors have accepted them?

**Dolma: An Open Corpus of Three Trillion Tokens for LLM Pretraining Research**

Dolma emphasizes reproducibility and transparency, but is full reproducibility realistically achievable at the trillion-token scale?