The Longformer paper: The Long-Document Transformer tackles a real limitation in standard transformer models, which is that the usual self-attention scales quadratically with the length of the input, making it very hard to deal with long texts. The authors Beltagy, Peters, and Cohan propose a new model called Longformer that solves this by using an attention mechanism that scales linearly with sequence length, combining local windowed attention with a small number of task-motivated global attention tokens. This lets the model process documents of thousands of tokens without running out of memory or compute.

Methodologically the paper replaces the full self-attention of typical transformers with this efficient hybrid of local and global attention, which is a drop-in replacement for standard self-attention. They pretrain the model and then fine-tune it on downstream tasks rather than just testing on toy problems, which gives the work a practical focus. The results show that Longformer achieves strong performance, setting new state-of-the-art on benchmarks like WikiHop and TriviaQA and also getting very competitive results in character-level language modeling tasks such as text8 and enwik8.

The goals of the research were clearly to allow transformers to scale to long documents while keeping high quality performance, and I think they achieved that. The methodology is solid, with both theoretical motivation and empirical evidence. The relevance of this work is high because being able to handle long contexts is important for real language tasks like summarization and QA that often involve long documents. Overall the paper is an important step forward in transformer research and its findings have influenced a lot of follow-on work in efficient attention models.