Questions

Paper 1 - "Scaling Laws for Neural Language Models" (Kaplan et al., 2020)
1.We see these very clean power-law curves for loss. But do you think thereâ€™s a point where the math breaks? For example, once a model hits a certain 'loss floor' , does adding more compute stop providing any useful reasoning improvements, even if the validation loss still looks like it's dropping?