
FlashAttention Paper: How does FlashAttentionâ€™s IO-aware design specifically reduce memory overhead compared to standard self-attention, and what implications does this have for scaling Transformers to very long sequences?

Longformer Paper: What performance does the Longformer achieve compared to prior models and works?