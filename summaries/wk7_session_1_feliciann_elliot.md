Questions

Paper 1 - Training language models to follow instructions with human feedback
1. RLHF improves helpfulness and safety but introduces minor regressions on some traditional NLP benchmarks (“alignment tax”). How can we design training objectives or RL methods to maximize alignment gains while minimizing performance trade-offs across tasks?

Paper 2 - Scaling Instruction-Finetuned Language Models
1. Based on Flan-PaLM’s human evaluation and compute analysis, how can instruction finetuning improve both the usability and cost-efficiency of deploying large language models in real-world applications?