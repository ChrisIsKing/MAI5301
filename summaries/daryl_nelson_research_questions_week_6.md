**ROFORMER Paper:** Beyond positional information, how might the rotational nature of RoPE influence the way information is represented in embedding space compared to additive encodings?


**TRAIN SHORT, TEST LONG Paper:** Based on how ALiBi biases attention with distance, what might be an implicit assumption about language or sequence data that this method leverages?